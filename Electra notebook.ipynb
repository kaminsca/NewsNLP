{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_data(tokenizer, data):\n",
    "    encoded_data = tokenizer.batch_encode_plus(\n",
    "        data['content'].tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "def create_dataset(encoded_data, data, device):\n",
    "    dataset = TensorDataset(\n",
    "        encoded_data['input_ids'].to(device),\n",
    "        encoded_data['attention_mask'].to(device),\n",
    "        torch.tensor(data['labels'].tolist()).float().to(device)\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def trainer(train_loss, val_losses, val_accuracies, num_epochs, train_loader, val_dataset, val_loader, model, optimizer, scheduler, device):\n",
    "    # Training Electra model\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"epoch {epoch}\", position=0):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if i % 250 == 0:\n",
    "                wandb.log({\"loss\": loss.item()})\n",
    "            if i % 2000 == 0:\n",
    "\n",
    "        # Evaluate the model\n",
    "                model.eval()\n",
    "                val_loss, val_acc, val_f1m, val_steps = 0, 0, 0, 0\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm(val_loader, total=len(val_loader), desc='training_eval', position=0):\n",
    "                        batch = tuple(t.to(device) for t in batch)\n",
    "                        input_ids, attention_mask, labels = batch\n",
    "                        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                        v_loss = outputs[0]\n",
    "                        logits = outputs[1]\n",
    "                        preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "                        \n",
    "                        ac = accuracy_score(labels.cpu().numpy().flatten(), preds.cpu().numpy().flatten())\n",
    "                        f1m = f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='micro')\n",
    "                        val_acc += ac\n",
    "                        val_f1m += f1m\n",
    "                        val_loss += v_loss.item()\n",
    "                        val_steps += 1\n",
    "        \n",
    "                    avg_val_loss = val_loss / val_steps\n",
    "                    avg_val_acc = val_acc / len(val_dataset)\n",
    "                    avg_val_f1m = val_f1m / len(val_dataset)\n",
    "\n",
    "                    wandb.log({\"avg_val_loss\": avg_val_loss, \"avg_val_acc\": avg_val_acc, \"avg_val_f1m\": avg_val_f1m})\n",
    "                    \n",
    "                    train_loss.append(v_loss.item())\n",
    "                    val_losses.append(avg_val_loss)\n",
    "                    val_accuracies.append(avg_val_acc)\n",
    "            \n",
    "                    print(\"\\n--------------------------------------------\")\n",
    "                    print('Epoch {:} / {:}'.format(epoch + 1, num_epochs))\n",
    "                    print(\"Training loss: \", loss.item())\n",
    "                    print(\"Validation loss: \", avg_val_loss)\n",
    "                    print(\"Validation accuracy: \", avg_val_acc)\n",
    "                    print(\"Validation f1m: \", avg_val_f1m)\n",
    "\n",
    "def testing(model, test_loader, test_data, device):\n",
    "    start = time.time()\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    test_loss = 0\n",
    "    test_steps = 0\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for batch in test_loader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            test_loss += loss.item()\n",
    "            # acc += (logits.argmax(1) == labels).sum().item()\n",
    "            acc += accuracy_score(labels.cpu().detach().numpy(), logits.cpu().detach().numpy())\n",
    "            test_steps += 1\n",
    "\n",
    "        accuracy = acc / len(test_data)\n",
    "\n",
    "    print(\"Test loss\", test_loss / test_steps)\n",
    "    print(\"Test accuracy: {:.2f}%\".format(accuracy*100))\n",
    "    print(\"Time\",time.time()-start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ELECTRA model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "\n",
    "# Load Electra Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google/electra-base-discriminator\",\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=3)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset SST-2 English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_data = pd.read_csv(\"./data/train.csv\", delimiter='|')\n",
    "val_data = pd.read_csv(\"./data/val.csv\", delimiter='|')\n",
    "test_data = pd.read_csv(\"./data/test.csv\", delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_data = encoded_data(tokenizer, train_data)\n",
    "val_encoded_data = encoded_data(tokenizer, val_data)\n",
    "test_encoded_data = encoded_data(tokenizer, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"avg_white_pop_pct\",\"avg_median_hh_inc\",\"avg_non_college_pct\"]\n",
    "train_data['labels'] = train_data[columns].values.tolist()\n",
    "val_data['labels'] = val_data[columns].values.tolist()\n",
    "test_data['labels'] = test_data[columns].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_encoded_data, train_data, device)\n",
    "val_dataset = create_dataset(val_encoded_data, val_data, device)\n",
    "test_dataset = create_dataset(test_encoded_data, test_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer and learning rate scheduler\n",
    "num_epochs = 5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=6.68561343998775e-5, eps=1e-8)\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"news-nlp\",\n",
    "\n",
    "    config={\n",
    "        \"epochs\": 5,\n",
    "        \"model\": 'electra',\n",
    "    }\n",
    ")\n",
    "wandb_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Electra model\n",
    "train_loss = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "start = time.time()\n",
    "trainer(train_loss, val_losses, val_accuracies, num_epochs, train_loader, val_dataset, val_loader, model, optimizer, scheduler, device)\n",
    "print(\"Time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "wandb_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'electra_models/transformerELECTRA-1.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google/electra-base-discriminator\", num_labels=2)\n",
    "model.load_state_dict(torch.load('electra_models/transformerELECTRA-1.pt', map_location=device))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(model, test_loader, test_data, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
